### ----- README
###
### Usage: python3 main.py MODEL_STR
###
### Supported MODEL_STR:
###     gpt3.5
###     gpt4
###     llama2
###     palm
###     bard  (gets converted to palm)
###
### Requirements:
### pip install openai
### pip install replicate
### pip install google-generativeai
###
### Make sure PROTOCOL variable is set to correct protocol. One of:
###     2pc
###     paxos
###     raft
###
### For Llama-2, first run:
###     export REPLICATE_API_TOKEN=<token_id_here>
### in terminal to set environment variable.
### Get API key from: https://replicate.com/account/api-tokens
###
### -----
### Reference/Documentation:
### Llama-2: https://replicate.com/blog/run-llama-2-with-an-api
### PaLM (Bard): https://developers.generativeai.google/tutorials/text_quickstart
###
### ----- END OF README


import openai  # GPT
import replicate  # Llama-2
import google.generativeai as palm  # PaLM (Bard)
import os
import sys

openai.api_key = "TODO: Enter OpenAI API key here"
palm.configure(api_key='TODO: Enter PaLM API key here')

# palm API offers:
#     'models/chat-bison-001'
#     'models/text-bison-001'
#     'models/embedding-gecko-001'
# 'models/text-bison-001' is the only one with 'generateText' in m.supported_generation_methods
models = {
    'gpt4': 'gpt-4',
    'gpt3.5': 'gpt-3.5-turbo',
    'llama2': 'llama-2',
    'palm': 'models/text-bison-001',
    'bard': None,  # 'bard' gets converted to 'palm'
}
MODEL = ''
MODEL_STR = ''
PROTOCOL = '2pc'
NUM_MUTATIONS = 10

prompt_sys = 'You are a TLA+ code reviewer. Your job is to review the TLA+ codes, and answer whether the code is correctly implemented.'
prompt_user = f'Review the following code. Then answer that whether it correctly implements the {PROTOCOL.capitalize()} protocol. Answering YES or NO and then explain why.'


def ask_palm(code):
    def palm_to_string(completion):
        return completion.result

    completion = palm.generate_text(
        model=MODEL,
        prompt=prompt_sys+'\n'+prompt_user+'\n\n'+code,
        temperature=0,
        # The maximum length of the response
        max_output_tokens=1000,
    )

    return palm_to_string(completion)

def ask_gpt(code):
    def openai_to_string(completion):
        return completion["choices"][0]["message"]["content"]

    completion = openai.ChatCompletion.create(
        model=MODEL,
        messages=[
                {"role": "system", "content": prompt_sys},
                {"role": "user", "content": prompt_user + "\n\n" + code},
        ],
        temperature=0
    )
    return openai_to_string(completion)

def ask_llama2(code):
    def repl_to_string(completion):
        output_str = ''
        for word in completion:
            output_str += word
        return output_str

    completion = replicate.run(
        "meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3",
        input={
            "prompt": prompt_sys+'\n'+prompt_user+'\n\n'+code
        }
    )

    return repl_to_string(completion)

def ask(code):
    if MODEL_STR in ['gpt3.5', 'gpt4']:
        return ask_gpt(code)
    elif MODEL_STR in ['llama2']:
        return ask_llama2(code)
    elif MODEL_STR in ['palm']:
        return ask_palm(code)
    else:
        print(f'ERROR: unrecognized model "{MODEL_STR}"')
        exit()


###
### Main function
###
if __name__ == "__main__":
    ###
    ### Validate command line arguments
    ###
    if len(sys.argv) != 2:
        print('Model name required. Currently supports:')
        for model_str, model in models.items():
            print(model_str)
        print('Usage: python3 main.py MODEL_STR')
        exit()
    elif sys.argv[1] not in models:
        print(f'invalid model: {sys.argv[1]}')
        print('Currently supports:')
        for model_str, model in models.items():
            print(model_str)
        exit()
    else:
        MODEL_STR = sys.argv[1] if sys.argv[1] != 'bard' else 'palm'
        MODEL = models[MODEL_STR]

    ###
    ### Mutations
    ###
    # Multiple runs (run 4 times)
    # for run in range(1, 5): 
    #     with open(f'{MODEL_STR}_{PROTOCOL}_results_{run}.txt', 'w') as file:

    # Single run
    with open(f'{MODEL_STR}_{PROTOCOL}_results_.txt', 'w') as file:
        file.write(f'Results generated by: {MODEL_STR}\n\n')
        if MODEL_STR in ['gpt3.5', 'gpt4']:
            file.write(f'System Prompt:\n{prompt_sys}\n')
            file.write(f'User Prompt  :\n{prompt_user}\n')
        else:
            prompt = prompt_sys+'\n'+prompt_user
            file.write(f'Prompt:\n{prompt}\n')
        file.write('\n----------------------------------------\n')

        for i in range(NUM_MUTATIONS):
            num = i + 1
            with open(f'{PROTOCOL}_mod{num}.txt', 'r') as code_file:
                code = code_file.read()

            response = ask(code)

            file.write(f'/{PROTOCOL}_mod{num}.txt\n')
            file.write(response)
            file.write('\n\n----------------------------------------\n')

    # ###
    # ### Correct
    # ###
    # with open(f'{MODEL_STR}_{PROTOCOL}_correct.txt', 'r') as code_file:
    #     code = code_file.read()
    # response = ask(code)
    # print(response)
